# R Script for High-Performance Data Analysis in Colab/R Environment
# Covering Arrow, Polars (via Reticulate), DuckDB, and Duckplyr for CSV to Parquet Workflow.

# --- 1. SETUP AND INSTALLATION ---

# Install core system dependencies (often pre-installed in Colab)
install.packages(c("Rcpp", "BH", "DBI"), dependencies = TRUE)

# 1. Install high-performance I/O and data manipulation libraries
message("Installing arrow, duckdb, duckplyr, and tidyverse...")
# Note: 'polars' is complex to install directly from CRAN in Google Colab; we'll use reticulate for Python Polars.
install.packages(c("arrow", "duckdb", "duckplyr", "tidyverse", "reticulate"), dependencies = TRUE)
install.packages("readr") # Added for completeness, though not strictly required by the core logic

# --- 2. LOAD LIBRARIES ---

# Load necessary R libraries
library(arrow)
library(duckdb)
library(duckplyr)
library(tidyverse)
library(reticulate)
library(ggplot2) # Explicitly load ggplot2
library(readr)

# --- 3. DEFINE PATHS ---

csv_path <- "/content/sample_data/california_housing_test.csv"
# Ensure unique output path for Parquet files generated by R/Arrow
parquet_path_arrow <- "/content/sample_data/california_housing_test_arrow.parquet"
# Ensure unique output path for Parquet files referenced by DuckDB
parquet_path_duckdb <- "/content/sample_data/california_housing_test_duckdb.parquet"


# ====================================================================
# --- PART A: ARROW WORKFLOW (CSV & PARQUET -> ANALYSIS) ---
# ====================================================================

message("\n--- Running Arrow Workflow (CSV to Parquet and Analysis) ---")

# Convert CSV to Parquet using Arrow's efficient native I/O
arrow_table <- read_csv_arrow(csv_path)
write_parquet(arrow_table, parquet_path_arrow)
print(paste("Parquet file created by Arrow at:", parquet_path_arrow))

# Read and analyse Parquet as an Arrow Dataset
dataset <- open_dataset(parquet_path_arrow)

# Use dplyr syntax on the Arrow Dataset (Zero-copy operations)
analysis_data <- dataset %>%
  select(longitude, latitude, median_house_value) %>%
  filter(median_house_value < 500001) %>%
  collect() # Pull results into R memory (tibble)

# Plotting the results
plot_arrow <- analysis_data %>%
  ggplot(aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point(alpha = 0.4) +
  scale_color_viridis_c() +
  labs(title = "Arrow Analysis: Median House Value Geographic Distribution",
       caption = "Data processed using the Arrow R Package") +
  theme_minimal()

print(plot_arrow)


# ====================================================================
# --- PART B: POLARS WORKFLOW (VIA RETICULATE) ---
# ====================================================================

message("\n--- Running Polars Workflow (via Reticulate) ---")

# 1. Install and import Python Polars, plus necessary S3 libraries
reticulate::py_install(packages = c("polars", "s3fs", "fsspec"), pip = TRUE)
pl <- import("polars")
# Optional: Setup SageMaker session (for specific AWS environments)
# reticulate::py_install("sagemaker")
# sagemaker <- import("sagemaker")
# session <- sagemaker$Session

# Placeholder for an S3 CSV path
s3_csv_path <- "s3://path/folder/file.csv"

# Read CSV from S3 (Requires s3fs/fsspec setup in the Python environment)
# NOTE: This line will fail if the S3 path is not valid or S3 credentials are not configured.
# df <- pl$read_csv(s3_csv_path) 

# --- Analysis (Demonstration using the Polars API) ---

# Since S3 read might fail, demonstrating Polars API assuming 'df' is available:
# df$dim()
# df$schema
# df$describe()

# Custom aggregations
# df_sales_summary <- df$select(
#   pl$col("sales")$sum()$alias("Total_Sales"),
#   pl$col("sales")$mean()$alias("Average_Sales"),
#   pl$col("sales")$median()$alias("Median_Sales")
# )
# print(df_sales_summary)

# Grouped aggregations
# df_grouped_summary <- df$group_by("category")$agg(
#   pl$col("sales")$mean()$alias("Avg_Category_Sales"),
#   pl$col("sales")$max()$alias("Max_Category_Sales")
# )
# print(df_grouped_summary)

# Convert Polars DataFrame to R tibble for ggplot2 (if df was read successfully)
# df_r_tibble <- df$collect()$to_r()
# print(head(df_r_tibble))
# df_r_tibble %>% ggplot(aes(x=..., y=...)) + ...


# ====================================================================
# --- PART C: DUCKDB/DUCKPLYR WORKFLOW (I/O & ANALYSIS) ---
# ====================================================================

message("\n--- Running DuckDB/Duckplyr Workflow (I/O and Analysis) ---")

# ----------------- C1: CSV to Parquet Conversion (I/O) -----------------

con_io <- dbConnect(duckdb::duckdb())
# Execute SQL COPY to convert CSV to Parquet using DuckDB's native I/O
dbExecute(con_io, 
          sprintf("COPY (SELECT * FROM read_csv_auto('%s')) TO '%s' (FORMAT PARQUET, OVERWRITE TRUE);",
                  csv_path, parquet_path_duckdb))
dbDisconnect(con_io, shutdown = TRUE)
print(paste("Parquet file created by DuckDB at:", parquet_path_duckdb))


# ----------------- C2: DuckDB SQL Analysis (CSV & Parquet) -----------------

sql_query_template <- "
  SELECT
    ROUND(longitude, 1) AS lon_group,
    AVG(median_house_value) AS avg_price,
    SUM(population) AS total_population
  FROM '%s'
  GROUP BY lon_group
  HAVING total_population > 1000
  ORDER BY lon_group;
"

# --- Analyze CSV using DuckDB SQL ---
con_sql_csv <- dbConnect(duckdb::duckdb())
csv_query <- sprintf(sql_query_template, csv_path)
csv_analysis_data <- dbGetQuery(con_sql_csv, csv_query)
dbDisconnect(con_sql_csv, shutdown = TRUE)

# Plot CSV Analysis
plot_sql_csv <- csv_analysis_data %>%
  ggplot(aes(x = lon_group, y = avg_price, size = total_population, color = avg_price)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(name = "Avg. Price") +
  labs(title = "DuckDB SQL Analysis: CSV (Avg. Price vs Longitude)", x = "Longitude", y = "Avg. Median House Value") +
  theme_minimal()
print(plot_sql_csv)

# --- Analyze Parquet using DuckDB SQL ---
con_sql_parquet <- dbConnect(duckdb::duckdb())
parquet_query <- sprintf(sql_query_template, parquet_path_duckdb)
parquet_analysis_data <- dbGetQuery(con_sql_parquet, parquet_query)
dbDisconnect(con_sql_parquet, shutdown = TRUE)

# Plot Parquet Analysis
plot_sql_parquet <- parquet_analysis_data %>%
  ggplot(aes(x = lon_group, y = avg_price, size = total_population, color = avg_price)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(name = "Avg. Price") +
  labs(title = "DuckDB SQL Analysis: Parquet (Avg. Price vs Longitude)", x = "Longitude", y = "Avg. Median House Value") +
  theme_minimal()
print(plot_sql_parquet)


# ----------------- C3: Duckplyr Analysis (CSV & Parquet) -----------------

# --- Duckplyr Analysis on CSV ---
con_dp_csv <- dbConnect(duckdb::duckdb())
# Register CSV as a temporary view within the connection
dbExecute(con_dp_csv, sprintf("CREATE OR REPLACE VIEW csv_data_view AS SELECT * FROM read_csv_auto('%s');", csv_path))

# Use tbl() to create a lazy reference to the view, enabling Duckplyr acceleration
csv_data_duckplyr <- tbl(con_dp_csv, "csv_data_view")

# Execute accelerated dplyr pipeline
csv_duckplyr_summary <- csv_data_duckplyr %>%
  mutate(lon_group = round(longitude, 1)) %>%
  group_by(lon_group) %>%
  summarise(
    avg_price = mean(median_house_value),
    total_population = sum(population)
  ) %>%
  filter(total_population > 1000) %>%
  ungroup() %>%
  collect() 

dbDisconnect(con_dp_csv, shutdown = TRUE)

# Plot CSV Duckplyr Analysis
plot_csv_duckplyr <- csv_duckplyr_summary %>%
  ggplot(aes(x = lon_group, y = avg_price, size = total_population, color = avg_price)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(name = "Avg. Price") +
  labs(title = "Duckplyr Analysis: CSV (Avg. Price vs Longitude)", x = "Longitude", y = "Avg. Median House Value") +
  theme_minimal()
print(plot_csv_duckplyr)


# --- Duckplyr Analysis on Parquet ---
con_dp_parquet <- dbConnect(duckdb::duckdb())
# Register Parquet as a temporary view
dbExecute(con_dp_parquet, sprintf("CREATE OR REPLACE VIEW parquet_data_view AS SELECT * FROM read_parquet('%s');", parquet_path_duckdb))

# Use tbl() to create a lazy reference to the Parquet view
parquet_data_duckplyr <- tbl(con_dp_parquet, "parquet_data_view")

# Execute accelerated dplyr pipeline
parquet_duckplyr_summary <- parquet_data_duckplyr %>%
  mutate(lon_group = round(longitude, 1)) %>%
  group_by(lon_group) %>%
  summarise(
    avg_price = mean(median_house_value),
    total_population = sum(population)
  ) %>%
  filter(total_population > 1000) %>%
  ungroup() %>%
  collect() 

dbDisconnect(con_dp_parquet, shutdown = TRUE)

# Plot Parquet Duckplyr Analysis
plot_parquet_duckplyr <- parquet_duckplyr_summary %>%
  ggplot(aes(x = lon_group, y = avg_price, size = total_population, color = avg_price)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(name = "Avg. Price") +
  labs(title = "Duckplyr Analysis: Parquet (Avg. Price vs Longitude)", x = "Longitude", y = "Avg. Median House Value") +
  theme_minimal()

print(plot_parquet_duckplyr)
